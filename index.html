<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Fast feed-forward text-to-3D">
  <meta name="keywords" content="Carve3D, Text-to-3D">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Carve3D: Improving Multi-view Reconstruction Consistency for Diffusion Models with RL Finetuning</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <script async src="https://unpkg.com/es-module-shims@1.8.0/dist/es-module-shims.js"></script>

  <script type="importmap">
    {
      "imports": {
        "three": "https://unpkg.com/three@0.156.1/build/three.module.js",
        "three/controls/OrbitControls": "https://unpkg.com/three@0.156.1/examples/jsm/controls/OrbitControls.js",
        "three/libs/stats": "https://unpkg.com/three@0.156.1/examples/jsm/libs/stats.module.js"
      }
    }
  </script>
  <script type="module" src="https://ajax.googleapis.com/ajax/libs/model-viewer/3.1.1/model-viewer.min.js"></script>
  <style>
    model-viewer {
      width: 300px;
      height: 300px;
    }
    .update {
      background-color: #f7f093;
      border-radius: 5pt;
      padding: 0.1em 1em;
    }
  </style>  
</head>

<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"> 
            <span class="dnerf">Carve3D: Improving Multi-view Reconstruction Consistency for Diffusion Models with RL Finetuning</span>
          </h1>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://desaixie.github.io/">Desai Xie</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="https://jiahao.ai">Jiahao Li</a><sup>1,3</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.unc.edu/~airsplay/">Hao Tan</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.sunxin.name/">Xin Sun</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://zhixinshu.github.io/">Zhixin Shu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://zhouyisjtu.github.io/">Yi Zhou</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://sai-bi.github.io/">Sai Bi</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://storage.googleapis.com/pirk.io/index.html">SÃ¶ren Pirk</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="https://www3.cs.stonybrook.edu/~ari/">Arie E. Kaufman</a><sup>2</sup>,
            </span>
<!--            <span class="author-block">Anonymous Authors</span> -->
            <p><br></p>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
		    <sup>1</sup><a href="https://research.adobe.com/">Adobe Research</a> &nbsp;&nbsp;
		    <sup>2</sup><a href="https://www.stonybrook.edu/">Stony Brook University</a> &nbsp;&nbsp;
		    <sup>3</sup><a href="https://ttic.edu/">TTIC</a> &nbsp;&nbsp;
		    <sup>4</sup><a href="https://www.uni-kiel.de/en/">Kiel University</a> &nbsp;&nbsp;
	        </span>
          </div>
          <div class="is-size-5 publication-authors">
            <p><br></p>
            <span class="author-block">
<!--              Keywords: Text-to-3D, diffusion model, RL finetuning for alignment, multi-view consistency. <br>-->
              Finetunes multi-view diffusion models with RL to improve multi-view consistency and NeRF quality, without relying on ground truth multi-view dataset. <br>
              Plus a novel metric for evaluating the consistency of multi-view diffusion models.
            </span>
          </div>

          <p><br></p>

          <div class="update">
            <p><b>UPDATE 02/26/2024:</b> Carve3D has been accepted to <a href="https://cvpr.thecvf.com/Conferences/2024">CVPR 2024</a>!</p>
            <p><b>UPDATE 03/04/2024:</b> Video results are available.</p>
            <p><b>UPDATE 04/14/2024:</b> Updated this website and our arXiv paper to include changes in the CVPR 2024 camera ready version.</p>
          </div>


          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2312.13980"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
<!--                  <span class="icon">-->
<!--                      <i class="fas fa-file-pdf"></i>-->
<!--                  </span>-->
                  <span>arxiv</span>
                </a>
              </span>
              <span class="link-block">
                  <a href="https://github.com/desaixie/carve3d" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <svg class="svg-inline--fa fa-github fa-w-16" aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" data-fa-i2svg=""><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg><!-- <i class="fab fa-github"></i> Font Awesome fontawesome.com -->
                    </span>
                    <span>Code & Data</span>
                  </a>
                </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
<!--    <div class="hero-body" style="display: flex; justify-content: center;">-->
<!--      <img src="./static/images/figure_teaser.png" style="width: 80%; border: none;" alt="Teaser Figure">-->
<!--    </div>-->

    <div class="hero-body" style="display: flex; justify-content: center;">
      <video id="video1" autoplay muted loop playsinline style="width: 70%; height: auto">
        <source src="./static/videos/figure_teaser_vid_trimmed.mp4" type="video/mp4">
      </video>
    </div>
    <p><strong>Figure 1</strong>.
      Our Carve3D algorithm steadily improves the 3D consistency of a multi-view diffusion model and the resulting quality
      of the NeRF and the mesh, without sacrificing its image-prompt
      alignment, texture details, or realism. Here, we show 3 testingset results (in 3 rows, numbered as 1-3, separated by dotted lines)
      from the finetuning process (epoch 0, 28, and 55 in 3 columns).
      Each row includes the generated multi-view images (denoted as
      MV), video and a sample frame of the reconstructed NeRF and extracted mesh (frame is denoted as RM and video is denoted as RMV)
      and the text prompt (denoted as TP). The inconsistencies in the
      multi-view images, e.g. the facing direction of the shopping cart,
      the position of the octopus arms, and the position of the pencils,
      lead to artifacts in the NeRF and the mesh (highlighted in red).</p>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Multi-view diffusion models, obtained by applying Supervised Finetuning (SFT) to text-to-image diffusion models, have driven recent breakthroughs in text-to-3D research. However, due to the limited size and quality of existing 3D datasets, they still suffer from multi-view inconsistencies and Neural Radiance Field (NeRF) reconstruction artifacts. We argue that multi-view diffusion models can benefit from further Reinforcement Learning Finetuning (RLFT), which allows models to learn from the data generated by themselves and improve beyond their dataset limitations during SFT. To this end, we introduce Carve3D, an improved RLFT algorithm coupled with a novel Multi-view Reconstruction Consistency (MRC) metric, to enhance the consistency of multi-view diffusion models. To measure the MRC metric on a set of multi-view images, we compare them with their corresponding NeRF renderings at the same camera viewpoints. The resulting model, which we denote as Carve3DM, demonstrates superior multi-view consistency and NeRF reconstruction quality than existing models. Our results suggest that pairing SFT with Carve3D's RLFT is essential for developing multi-view-consistent diffusion models, mirroring the standard Large Language Model (LLM) alignment pipeline.
          </p>
          <p><br></p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <h2 class="title is-3" style="display: flex; justify-content: center;">Overview</h2>
    <div class="hero-body" style="display: flex; justify-content: center;">
      <img src="./static/images/figure_overview.png" style="width: 100%; border: none;" alt="Teaser Figure">
    </div>
    <p><strong>Figure 2</strong>. Overview of Carve3D. Given a prompt sampled from our curated prompt set and a initial noisy image, we iteratively denoise
      the image using the UNet. The final, clean image contains four multi-view images tiled in a 2-by-2 grid. MRC reward is computed by
      comparing (a) the generated multi-view images with (c) the corresponding multi-view images rendered at the same camera viewpoints
      from (b) the reconstructed NeRF. Then, we train the model with policy gradient loss function, where the loss is derived from the reward
      and log probabilities of the UNetâs predictions, accumulated over all denoising timesteps. By using only a set of training text prompts, our
      RLFT algorithm finetunes the diffusion model by evaluating its own generated outputs, without relying on ground truth multi-view images.
    </p>
    <p style="margin-bottom:2cm;"><br></p>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <h2 class="title is-3" style="display: flex; justify-content: center;">Improving Multi-view Consistency and NeRF Quality</h2>
<!--    <div class="hero-body" style="display: flex; justify-content: center;">-->
<!--      <img src="./static/images/figure_suppl_qualitative.png" style="width: 100%; border: none;" alt="Teaser Figure">-->
<!--    </div>-->
    <div class="hero-body" style="display: flex; justify-content: center;">
      <video id="video2" autoplay muted loop playsinline style="width: 100%; height: auto">
        <source src="./static/videos/figure_suppl_qualitative_vid_trimmed_doubled.mp4" type="video/mp4">
      </video>
    </div>
    <p><strong>Figure 3</strong>.  Qualitative comparison of Instant3D (the base model) and Carve3D (ours, finetuned from Instant3D) on 12 prompts (in 12
      blocks, numbered as 1-12, separated by dotted line). In each block, we show the their generated multi-view images in the 2-by-2 grid
      (denoted as MV), video and a sample frame of the reconstructed NeRF and extracted mesh (frame is denoted as RM and video is denoted as RMV) when given the text prompt (denoted as TP). For each
      result, we use the same randomly sampled initial noise for all models to ensure the comparison is fair. We draw red boxes on the NeRF
      and the extracted mesh to highlight the artifacts in the NeRF and the mesh, resulting from the inconsistencies in the multi-view images.
      Carve3D maintains the detailed texture and provides improved multi-view consistency and higher quality NeRF than the base Instant3D.
    </p>
    <p style="margin-bottom:2cm;"><br></p>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <h2 class="title is-3" style="display: flex; justify-content: center;">Comparing to Base Model with Longer SFT and Existing Methods</h2>
    <div class="hero-body" style="display: flex; justify-content: center;">
      <img src="./static/images/figure_rl_vs_sft.png" style="width: 100%; border: none;" alt="Teaser Figure">
    </div>
    <p><strong>Figure 4</strong>.  Qualitative comparison of MVDream [46], Instant3D [24] with 10K (the base model), 20K, and 100K SFT steps, Carve3D (ours,
      finetuned from Instant3D-10K), Zero123++ [47], and SyncDreamer [28] (7 models in 7 columns) on 4 prompts (in 4 rows, numbered as 1-4,
      separated by dotted line). In each row, we show their generated multi-view images in the 2-by-2 grid (denoted as MV), reconstructed NeRF
      and extracted mesh (denoted as RM) when given the text prompt (denoted as TP). MVDream, Zero123++, and SyncDreamer generates
      inconsistent multi-view images and reconstruction artifacts (highlighted in red). For each result, we use the same randomly sampled initial
      noise for all models to ensure the comparison is fair. We let Zero123++ and SyncDreamer to use one of Carve3Dâs output multi-view
      images as their input image conditioning. Instant3D-10K, -20K, and -100K and Carve3D demonstrates progressively better multi-view
      consistency and reconstruction quality. Instant3D-10K, Carve3D, Zero123++, and SyncDreamer exhibits the best texture details and
      realism, whereas Instant3D-20K and -100K with prolonged SFT steps compromise those qualities.
    </p>
    <p style="margin-bottom:2cm;"><br></p>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <h2 class="title is-3"><center>Maintaining Diversity</center></h2>
    <div class="hero-body" style="display: flex; justify-content: center;">
      <img src="./static/images/figure_diversity.png" style="width: 100%; border: none;" alt="Teaser Figure">
    </div>
    <p><strong>Figure 5</strong>. Qualitative comparison of Instant3D (the base model) and Carve3D (ours, finetuned from Instant3D) on 4 prompts (in 4 rows,
      numbered as 1-4, separated by the dotted line) demonstrating diversity. In each row, we show 3 results from each model, including the
      generated multi-view images in the 2-by-2 grid (denoted as MV), the reconstructed NeRF and the extracted mesh (denoted as bottom)
      when given the prompt (denoted as middle). For each result, we use the same randomly sampled initial noise for all models to ensure the
      comparison is fair. Our RLFT maintains the diversity of the base Instant3D model, while improving the consistency.
    </p>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{xie2023carve3d,
    title={Carve3D: Improving Multi-view Reconstruction Consistency for Diffusion Models with RL Finetuning},
    author={Desai Xie and Jiahao Li and Hao Tan and Xin Sun and Zhixin Shu and Yi Zhou and Sai Bi and SÃ¶ren Pirk and Arie E. Kaufman},
    year={2023},
    eprint={2312.13980},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            We borrow the source code of this website from <a
              href="https://github.com/nerfies/nerfies.github.io">HERE</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
